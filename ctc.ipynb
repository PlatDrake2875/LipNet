{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "\n",
    "# === GLOBAL CONFIGURATION ===\n",
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 40\n",
    "BATCH_SIZE = 96\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_WORKERS = 0  # No parallel data loading workers -> THEY DONT WORK WITH JUPYTER!\n",
    "\n",
    "# --------------------------------------------------\n",
    "# CTC REQUIRES A BLANK LABEL\n",
    "# We'll define '_' (underscore) as the blank token (index 0).\n",
    "# The rest of the symbols follow after.\n",
    "# --------------------------------------------------\n",
    "LABELS = \"_abcdefghijklmnopqrstuvwxyz' \"  # 1st char '_' is for blank\n",
    "BLANK_IDX = 0\n",
    "LABEL2IDX = {label: idx for idx, label in enumerate(LABELS)}\n",
    "IDX2LABEL = {idx: label for label, idx in LABEL2IDX.items()}\n",
    "\n",
    "# === VERBOSE PRINT FUNCTION ===\n",
    "verbose = True\n",
    "def verbose_print(message):\n",
    "    if verbose:\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATASET DEFINITION ===\n",
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, wav_paths, align_paths):\n",
    "        self.wav_paths = wav_paths\n",
    "        self.align_paths = align_paths\n",
    "        self.mel_transform = MelSpectrogram(sample_rate=SAMPLE_RATE, n_mels=N_MELS)\n",
    "\n",
    "        verbose_print(f\"Initialized SpeechDataset with {len(self.wav_paths)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wav_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_path = self.wav_paths[idx]\n",
    "        align_path = self.align_paths[idx]\n",
    "\n",
    "        # Load audio and resample\n",
    "        audio, _ = librosa.load(wav_path, sr=SAMPLE_RATE)\n",
    "        \n",
    "        # Extract Mel-spectrogram\n",
    "        audio_tensor = torch.tensor(audio).float()\n",
    "        features = self.mel_transform(audio_tensor)  # (n_mels, time)\n",
    "        features = features.transpose(0, 1)  # Make it (time, n_mels)\n",
    "\n",
    "        # Load alignment and convert to label indices\n",
    "        with open(align_path, 'r') as f:\n",
    "            alignment = f.read().strip().replace(\"\\n\", \" \")\n",
    "        # Convert each character to its label index if it exists in LABEL2IDX\n",
    "        labels = [LABEL2IDX[char] for char in alignment if char in LABEL2IDX]\n",
    "\n",
    "        return features, torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# === CUSTOM COLLATE FUNCTION ===\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: List of tuples (features, labels) from SpeechDataset.\n",
    "    Each 'features' is shape (T, n_mels).\n",
    "    Each 'labels' is shape (L,).\n",
    "    \"\"\"\n",
    "    features_list, labels_list = zip(*batch)\n",
    "\n",
    "    max_feat_len = max(feat.size(0) for feat in features_list)\n",
    "\n",
    "    padded_features = []\n",
    "    input_lengths = []\n",
    "    for feat in features_list:\n",
    "        seq_len = feat.size(0)\n",
    "        pad_len = max_feat_len - seq_len\n",
    "        feat_padded = F.pad(feat, (0, 0, 0, pad_len))  # pad time dimension\n",
    "        padded_features.append(feat_padded)\n",
    "        input_lengths.append(seq_len)\n",
    "\n",
    "    target_lengths = [len(lbl) for lbl in labels_list]\n",
    "    flattened_labels = torch.cat(labels_list)\n",
    "\n",
    "    padded_features = torch.stack(padded_features, dim=0)  # (batch, max_time, n_mels)\n",
    "    input_lengths = torch.tensor(input_lengths, dtype=torch.long)\n",
    "    target_lengths = torch.tensor(target_lengths, dtype=torch.long)\n",
    "\n",
    "    return padded_features, flattened_labels, input_lengths, target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL DEFINITION ===\n",
    "class CTCModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\n",
    "        super(CTCModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, \n",
    "            hidden_dim, \n",
    "            batch_first=True, \n",
    "            bidirectional=True,\n",
    "            dropout=dropout,\n",
    "            num_layers=2  # 2-layer LSTM\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # bidirectional doubles hidden_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, time, input_dim)\n",
    "        returns: (batch, time, output_dim)\n",
    "        \"\"\"\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# === CTC LOSS WRAPPER ===\n",
    "def ctc_loss_fn(logits, targets, input_lengths, target_lengths):\n",
    "    \"\"\"\n",
    "    logits: (batch, time, num_classes)\n",
    "    targets: (N) 1D tensor of all targets concatenated\n",
    "    \"\"\"\n",
    "    logits = logits.permute(1, 0, 2)  # -> (time, batch, num_classes)\n",
    "    log_probs = F.log_softmax(logits, dim=2)\n",
    "    \n",
    "    ctc_loss = nn.CTCLoss(\n",
    "        blank=BLANK_IDX,\n",
    "        zero_infinity=True\n",
    "    )(log_probs, targets, input_lengths, target_lengths)\n",
    "\n",
    "    return ctc_loss\n",
    "\n",
    "# === TRAINING LOOP ===\n",
    "def train_model(model, dataloader, optimizer, num_epochs, device):\n",
    "    model.train()\n",
    "    total_steps = len(dataloader) * num_epochs  # total number of training steps\n",
    "    verbose_print(f\"[train_model] Total epochs: {num_epochs}, total steps: {total_steps}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        verbose_print(f\"[train_model] Starting epoch {epoch+1}/{num_epochs} (batches in epoch: {len(dataloader)})...\")\n",
    "        \n",
    "        for batch_idx, (features, labels, input_lengths, target_lengths) in enumerate(dataloader):\n",
    "            global_step = epoch * len(dataloader) + batch_idx  # current global step\n",
    "\n",
    "            # Move data to device\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            input_lengths = input_lengths.to(device)\n",
    "            target_lengths = target_lengths.to(device)\n",
    "\n",
    "            # Forward\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(features)\n",
    "\n",
    "            # Compute CTC loss\n",
    "            loss = ctc_loss_fn(logits, labels, input_lengths, target_lengths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Print epoch details for verbose\n",
    "            if verbose and (batch_idx % 25 == 0):\n",
    "                verbose_print(\n",
    "                    f\"  Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                    f\"Batch [{batch_idx}/{len(dataloader)}], \"\n",
    "                    f\"Global Step: {global_step}/{total_steps}, \"\n",
    "                    f\"Loss: {loss.item():.4f}, \"\n",
    "                    f\"Features shape: {features.shape}, \"\n",
    "                    f\"Labels shape: {labels.shape}\"\n",
    "                )\n",
    "\n",
    "        # Compute avg loss per batch\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        verbose_print(f\"Epoch [{epoch+1}/{num_epochs}] completed - Avg Loss: {avg_loss:.4f}\\n\")\n",
    "\n",
    "\n",
    "# === DECODE FUNCTION (GREEDY) ===\n",
    "def decode_ctc(logits):\n",
    "    \"\"\"\n",
    "    logits: (batch, time, num_classes)\n",
    "    Returns a list of decoded strings (one per batch element).\n",
    "    \"\"\"\n",
    "    argmax_indices = torch.argmax(logits, dim=2)  # (batch, time)\n",
    "    decoded_batch = []\n",
    "\n",
    "    for sequence in argmax_indices:\n",
    "        decoded_seq = []\n",
    "        prev_idx = None\n",
    "        for idx in sequence:\n",
    "            idx = idx.item()\n",
    "            # Skip repeating indices or blank\n",
    "            if idx != prev_idx and idx != BLANK_IDX:\n",
    "                decoded_seq.append(IDX2LABEL[idx])\n",
    "            prev_idx = idx\n",
    "        decoded_batch.append(\"\".join(decoded_seq))\n",
    "\n",
    "    return decoded_batch\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Helper function to gather data for speakers s1..s34,\n",
    "# excluding s1, s2, s20, s21, and s22 by default.\n",
    "# ------------------------------------------------------------------\n",
    "def gather_speaker_data(wav_root, align_root, exclude_speakers=None):\n",
    "    \"\"\"\n",
    "    Gathers .wav and .align file paths from speaker directories\n",
    "    (s1 through s34) but excludes certain speaker IDs if provided.\n",
    "    Returns two sorted lists of paths: (all_wav_paths, all_align_paths).\n",
    "    \"\"\"\n",
    "    if exclude_speakers is None:\n",
    "        exclude_speakers = [1, 2, 20, 21, 22]\n",
    "\n",
    "    all_wav_paths = []\n",
    "    all_align_paths = []\n",
    "\n",
    "    for spk_id in range(1, 35):\n",
    "        if spk_id in exclude_speakers:\n",
    "            verbose_print(f\"[gather_speaker_data] Skipping speaker s{spk_id} (excluded).\")\n",
    "            continue\n",
    "\n",
    "        spk_dir = f\"s{spk_id}\"\n",
    "        spk_wav_dir = os.path.join(wav_root, spk_dir)\n",
    "        spk_align_dir = os.path.join(align_root, spk_dir)\n",
    "\n",
    "        if not os.path.isdir(spk_wav_dir) or not os.path.isdir(spk_align_dir):\n",
    "            verbose_print(f\"[gather_speaker_data] Speaker s{spk_id} directory missing, skipping.\")\n",
    "            continue\n",
    "\n",
    "        verbose_print(f\"[gather_speaker_data] Collecting data for speaker s{spk_id}...\")\n",
    "\n",
    "        # Collect .wav files\n",
    "        spk_wavs = [\n",
    "            os.path.join(spk_wav_dir, fname)\n",
    "            for fname in os.listdir(spk_wav_dir)\n",
    "            if fname.endswith(\".wav\")\n",
    "        ]\n",
    "        # Collect .align files\n",
    "        spk_aligns = [\n",
    "            os.path.join(spk_align_dir, fname)\n",
    "            for fname in os.listdir(spk_align_dir)\n",
    "            if fname.endswith(\".align\")\n",
    "        ]\n",
    "\n",
    "        verbose_print(f\"  Found {len(spk_wavs)} wavs and {len(spk_aligns)} aligns in s{spk_id}.\")\n",
    "\n",
    "        all_wav_paths.extend(spk_wavs)\n",
    "        all_align_paths.extend(spk_aligns)\n",
    "\n",
    "    # Sort for consistent ordering\n",
    "    all_wav_paths.sort()\n",
    "    all_align_paths.sort()\n",
    "\n",
    "    return all_wav_paths, all_align_paths\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# MAIN FUNCTION that uses data from s1..s34 except s1, s2, s20, s21, s22\n",
    "# ------------------------------------------------------------------\n",
    "def main_excluding_some_speakers(wav_root, align_root):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    verbose_print(f\"Using device: {device}\")\n",
    "\n",
    "    wav_paths, align_paths = gather_speaker_data(\n",
    "        wav_root, align_root, \n",
    "        exclude_speakers=[1, 2, 20, 21, 22]\n",
    "    )\n",
    "    verbose_print(f\"Total .wav files collected: {len(wav_paths)}\")\n",
    "    verbose_print(f\"Total .align files collected: {len(align_paths)}\")\n",
    "\n",
    "    assert len(wav_paths) == len(align_paths), \"Number of .wav and .align files must match\"\n",
    "\n",
    "    dataset = SpeechDataset(wav_paths, align_paths)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True,\n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    input_dim = N_MELS\n",
    "    hidden_dim = 256\n",
    "    output_dim = len(LABELS)  # includes blank\n",
    "    model = CTCModel(input_dim, hidden_dim, output_dim, dropout=0.3)\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Train\n",
    "    train_model(model, dataloader, optimizer, num_epochs=NUM_EPOCHS, device=device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for features, labels, input_lengths, target_lengths in dataloader:\n",
    "            features = features.to(device)\n",
    "            logits = model(features)  # (batch, time, output_dim)\n",
    "            decoded_output = decode_ctc(logits)\n",
    "            verbose_print(\"[main_excluding_some_speakers] Predictions on a batch:\")\n",
    "            for pred_str in decoded_output:\n",
    "                print(\"  \", pred_str)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    wav_root = r\"GRID\\audio_25k\\audio_25k\"\n",
    "    align_root = r\"GRID\\alignments\\alignments\"\n",
    "    main_excluding_some_speakers(wav_root, align_root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
